{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating PMI Features\n",
    "\n",
    "The basic algorithm is as follows:\n",
    "\n",
    "1. Read in the data as pandas data frame  \n",
    "2. Combine all the essays together, remove HTML tags, remove newline characters  \n",
    "3. Tokenize all the essays using the happyfuntokenizer  \n",
    "4. Use the tokens to generate unigrams, bigrams, and trigrams. Stopwords are removed for bigrams and trigrams as well as unigrams. I felt that the bigrams and trigrams were more informative after this step. But we can add them back in if you'd like. These are stored in freqdists that come in handy for the PMI calculation.       \n",
    "5. Only keep unigrams that occur more than 3 times, bigrams with PMI > 4, and trigrams with PMI > 6. This starts building the vocabulary that will be used to vectorize the essay data.      \n",
    "6. Filter out unigrams, bigrams, and trigrams that are used by less than 1% of the users.   \n",
    "7. Use the CountVectorizer to vectorize the user data, making sure to remove stopwords, use the hft to tokenize, analyze using 1 to 3-grams, and the new vocabulary. The rows are normalized by the number of words a user uses in all of their essays.  \n",
    "8. The data matrix is written to a pickle file in case you don't want to run this again, but it doesn't take too long. Less than 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from utils import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking whether 'w not in stopwords.words('english') takes FOREVER. So I converted it to a dictionary\n",
    "#because checking whether or not w is in a list of size n takes O(n) time, but looking something up in\n",
    "#a dictionary takes O(1) time.\n",
    "stop_dict = {}\n",
    "for w in stopwords.words('english'):\n",
    "    stop_dict[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gets done in a reasonable time, joins all the essays together into one long text\n",
    "#Also gets rid of HTML tags using the BeautifulSoup Library\n",
    "def get_data():\n",
    "    df = pd.read_csv('data/profiles.20120630.csv')\n",
    "    \n",
    "    def remove_nan(s):\n",
    "        if type(s) == float:\n",
    "            return ''\n",
    "        return s\n",
    "    \n",
    "    #dealing with the nan values of essays\n",
    "    essays = df.columns.values[7:17]\n",
    "    for text in essays:\n",
    "        df[text] = df[text].apply(remove_nan)\n",
    "    df['TotalEssays'] = df[essays].apply(lambda x: ' '.join(x), axis=1)\n",
    "    df['TotalEssays'] = df['TotalEssays'].apply(lambda x: BeautifulSoup(x).getText().replace('\\n', ' '))\\\n",
    "                                         .apply(lambda x: re.sub('\\s+', ' ', x).strip())\n",
    "    return df[df.TotalEssays.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizes the concatenated essay using the hft\n",
    "def tokenize_words(df):\n",
    "    tokenizer = happyfuntokenizing.Tokenizer()\n",
    "    df['TotalEssayTokens'] = df['TotalEssays'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generates freqdists for unigrams, bigrams, and trigrams\n",
    "def generate_freqdists(df, stop_dict):\n",
    "    words = df['TotalEssayTokens'].tolist()\n",
    "    words = [item for sublist in words for item in sublist]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = [wordnet_lemmatizer.lemmatize(w.lower())\n",
    "             for w in words\n",
    "             if w not in string.punctuation and w.lower() not in stop_dict]\n",
    "    unigram_freq = nltk.FreqDist(words)\n",
    "    \n",
    "    bigrams = nltk.ngrams(words, 2)\n",
    "    bigram_freq = nltk.FreqDist(bigrams)\n",
    "    \n",
    "    trigrams = nltk.ngrams(words, 3)\n",
    "    trigram_freq = nltk.FreqDist(trigrams)\n",
    "    \n",
    "    return unigram_freq, bigram_freq, trigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generates a vocabularly for the count vectorizer by computing the pmi for bigrams and trigrams like the paper\n",
    "def generate_vocab(freq_dists):\n",
    "    vocab = []\n",
    "    unigram_freq = freq_dists[0]\n",
    "    for k in unigram_freq.keys():\n",
    "        if unigram_freq[k] >= 3:\n",
    "            vocab.append(k)\n",
    "    print(vocab[:10])\n",
    "    \n",
    "    vocab2 = []\n",
    "    bigram_freq = freq_dists[1]\n",
    "    unicount = sum(unigram_freq.values())\n",
    "    bicount = sum(bigram_freq.values())\n",
    "    for k in bigram_freq.keys():\n",
    "        num = bigram_freq[k] / bicount\n",
    "        denom = (unigram_freq[k[0]] / unicount) * (unigram_freq[k[1]] / unicount)\n",
    "        if num / denom > 4:\n",
    "            vocab2.append(k)\n",
    "    \n",
    "    vocab3 = []\n",
    "    trigram_freq = freq_dists[2]\n",
    "    tricount = sum(trigram_freq.values())\n",
    "    print(tricount)\n",
    "    for k in trigram_freq.keys():\n",
    "        num = trigram_freq[k] / tricount\n",
    "        denom = (unigram_freq[k[0]] / unicount) * (unigram_freq[k[1]] / unicount) * (unigram_freq[k[2]] / unicount)\n",
    "        if num / denom > 6:\n",
    "            vocab3.append(k)\n",
    "\n",
    "    return set(vocab + vocab2 + vocab3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filters out words from the vocabulary v that are used by less than 1% of the users in the dataframe\n",
    "def filter_vocab(df, v):\n",
    "    #Every word gets a dictionary entry\n",
    "    v_dict = {}\n",
    "    for w in v:\n",
    "        v_dict[w] = 0\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #Going through every essay's unigrams, bigrams, trigrams\n",
    "    #only adding to dictionary once if they exist in that essay\n",
    "    essays = df['TotalEssayTokens'].tolist()\n",
    "    for i,e in enumerate(essays):\n",
    "        words = [wordnet_lemmatizer.lemmatize(w.lower())\n",
    "                 for w in e\n",
    "                 if w not in string.punctuation and w.lower() not in stop_dict]\n",
    "        uni = words\n",
    "        bi = nltk.ngrams(words, 2)\n",
    "        tri = nltk.ngrams(words, 3)\n",
    "        total = set(list(uni) + list(bi) + list(tri)) #only counting once\n",
    "        for t in total:\n",
    "            if t in v_dict:\n",
    "                v_dict[t] += 1\n",
    "    #Removing words that occur in less than 599 different users           \n",
    "    final_vocab = []\n",
    "    for k in v_dict.keys():\n",
    "        if v_dict[k] >= 599:\n",
    "            if type(k) == tuple:\n",
    "                final_vocab.append(' '.join(k)) #joining bigrams and trigrams\n",
    "            else:\n",
    "                final_vocab.append(k)\n",
    "    return final_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates the data matrix, normalizes by user word count, writes the matrix to a pickle file just in case\n",
    "#returns data matrix if needed.\n",
    "def create_data_matrix(df, vocab, filename):\n",
    "    \n",
    "    count_vect = CountVectorizer(stop_words='english', tokenizer=happyfuntokenizing.Tokenizer().tokenize,\n",
    "                                 ngram_range=(1, 3), analyzer='word', vocabulary=vocab)\n",
    "    data_matrix = count_vect.fit_transform(df['TotalEssays'])\n",
    "    \n",
    "    data_matrix_dense = data_matrix.todense().astype(float)\n",
    "    \n",
    "    #Normalizing each row by the number of words that user uses\n",
    "    essays = main_df['TotalEssayTokens'].tolist()\n",
    "    for i,e in enumerate(essays):\n",
    "        data_matrix_dense[i, :] = data_matrix_dense[i, :] / len(e)\n",
    "            \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data_matrix_dense, f)\n",
    "        \n",
    "    return data_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df = tokenize_words(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"i'm\", 198042), ('like', 138954), ('love', 123892), ('...', 103644), ('friend', 95509), ('good', 89676), ('music', 83848), ('thing', 81601), ('time', 79769), ('people', 78147)]\n",
      "\n",
      "[(('san', 'francisco'), 12042), (('family', 'friend'), 10301), (('bay', 'area'), 10248), (('friend', 'family'), 9102), (('sense', 'humor'), 8658), (('new', 'thing'), 7419), (('pretty', 'much'), 6755), ((\"i'm\", 'pretty'), 6747), ((\"i'm\", 'really'), 6302), ((\"i'm\", 'looking'), 6218)]\n",
      "\n",
      "[(('making', 'people', 'laugh'), 3282), (('http', ':/', 'www'), 2616), (('spend', 'lot', 'time'), 2526), (('meeting', 'new', 'people'), 2468), ((\"i'm\", 'really', 'good'), 2159), (('trying', 'new', 'thing'), 2036), (('meet', 'new', 'people'), 1880), (('pretty', 'much', 'anything'), 1771), (('www', 'youtube', 'com'), 1582), (('typical', 'friday', 'night'), 1574)]\n"
     ]
    }
   ],
   "source": [
    "freq_dists = generate_freqdists(main_df, stop_dict)\n",
    "print(freq_dists[0].most_common(10))\n",
    "print()\n",
    "print(freq_dists[1].most_common(10))\n",
    "print()\n",
    "print(freq_dists[2].most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n00bs', 'contest', \"wnyc's\", 'catholicism', 'replant', 'rummage', 'xtc', 'outsider', 'discontent', 'habana']\n",
      "12620654\n"
     ]
    }
   ],
   "source": [
    "vocab = generate_vocab(freq_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = filter_vocab(main_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_matrix = create_data_matrix(main_df, vocab, 'datamatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data_matrix.todense()).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
