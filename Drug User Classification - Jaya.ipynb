{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from utils.clean_up import *\n",
    "from utils.categorize_demographics import *\n",
    "from utils.nonnegative_matrix_factorization import nmf_inspect, nmf_labels\n",
    "from utils.distinctive_tokens import log_odds_ratio\n",
    "from utils.classification import betas\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keeping track of the names of the essays\n",
    "essay_dict = {'essay0' : 'My self summary',\n",
    "              'essay1' : 'What I\\'m doing with my life',\n",
    "              'essay2' : 'I\\'m really good at',\n",
    "              'essay3' : 'The first thing people notice about me',\n",
    "              'essay4' : 'Favorite books, movies, tv, food',\n",
    "              'essay5' : 'The six things I could never do without',\n",
    "              'essay6' : 'I spend a lot of time thinking about',\n",
    "              'essay7' : 'On a typical Friday night I am',\n",
    "              'essay8' : 'The most private thing I am willing to admit',\n",
    "              'essay9' : 'You should message me if'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/profiles.20120630.csv')\n",
    "\n",
    "essay_list = ['essay0', 'essay4']\n",
    "df_0, df_4 = clean_up(df, essay_list)\n",
    "\n",
    "df_4 = recategorize(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>books: absurdistan, the republic, of mice and ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am die hard christopher moore fan. i don't r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>okay this is where the cultural matrix gets so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bataille, celine, beckett. . . lynch, jarmusch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>music: bands, rappers, musicians at the moment...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Category\n",
       "0  books: absurdistan, the republic, of mice and ...        -1\n",
       "1  i am die hard christopher moore fan. i don't r...         1\n",
       "2  okay this is where the cultural matrix gets so...         0\n",
       "3  bataille, celine, beckett. . . lynch, jarmusch...         0\n",
       "4  music: bands, rappers, musicians at the moment...        -1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating out the drug users, non drug users, and unknowns\n",
    "def get_category(text):\n",
    "    if 'unknown' in text:\n",
    "        return 0\n",
    "    if 'yes' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "main_df = df_4[['essay4']]\n",
    "main_df['Category'] = df_4['drugs'].apply(get_category)\n",
    "main_df.columns = ['Text', 'Category']\n",
    "no_df = main_df[main_df.Category == -1]\n",
    "no_df = no_df[:6859] # balancing the classes\n",
    "unknown_df = main_df[main_df.Category == 0]\n",
    "yes_df = main_df[main_df.Category == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book: janice dickinson: no lifeguard on duty. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>books: satipatthana by analayo, radical accept...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i like to read, and past favorite authors are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books: 1984 by george orwell, comics movies: a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- favorite books: the girl with the dragon tat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Category\n",
       "0  book: janice dickinson: no lifeguard on duty. ...         1\n",
       "1  books: satipatthana by analayo, radical accept...        -1\n",
       "2  i like to read, and past favorite authors are ...         1\n",
       "3  books: 1984 by george orwell, comics movies: a...        -1\n",
       "4  - favorite books: the girl with the dragon tat...        -1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating training data\n",
    "train_df = pd.concat([no_df, yes_df], axis=0) #stack the two together\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index)) #reshuffle them\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "unknown_df = unknown_df.reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REPLACE WITH NMF FEATURES\n",
    "def add_features(df):\n",
    "    #tokenize text\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "         ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "       | \\w+([-']\\w+)*        # words with optional internal hyphens\n",
    "       | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "       | [!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+  # these are separate tokens (string.punctuation)\n",
    "     '''\n",
    "    tokenize = lambda text: nltk.regexp_tokenize(text, pattern)\n",
    "    df['Tokens'] = df['Text'].apply(tokenize)\n",
    "    \n",
    "    def pos_tag(text):\n",
    "        tuples = nltk.pos_tag(text)\n",
    "        tags = []\n",
    "        for t in tuples:\n",
    "            tags.append(t[1])\n",
    "        return tags\n",
    "    \n",
    "    def get_nouns(text):\n",
    "        wnlemmatizer = nltk.WordNetLemmatizer()\n",
    "        tuples = nltk.pos_tag(text)\n",
    "        tags = []\n",
    "        for t in tuples:\n",
    "            if t[1][0] == 'N':\n",
    "                tags.append(wnlemmatizer.lemmatize(t[0]))\n",
    "        return tags\n",
    "    \n",
    "    df['Nouns'] = df['Tokens'].apply(get_nouns)\n",
    "    df['POS'] = df['Tokens'].apply(pos_tag)\n",
    "    make_string = lambda a: ' '.join(i for i in a)\n",
    "    df['Nouns'] = df['Nouns'].apply(make_string)\n",
    "    df['POS'] = df['POS'].apply(make_string)\n",
    "    \n",
    "    df['TokenString'] = df['Tokens'].apply(make_string)\n",
    "    \n",
    "    df['TokensNouns'] = df[['TokenString', 'Nouns']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorize data\n",
    "#turns words into a list of vectors - vector length is the total number of words\n",
    "#Vector elements correspond to 1 word (1/0 if word is/not present in the current item)\n",
    "def featurize(df, test_df, count_vec):\n",
    "    pos_vec = CountVectorizer()\n",
    "    noun_vec = CountVectorizer()\n",
    "    vec_tar = LabelEncoder()\n",
    "    targets = df['Category']\n",
    "        \n",
    "    counts = count_vec.fit_transform(df['Text'])\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(counts)\n",
    "    \n",
    "    \n",
    "    test = count_vec.transform(test_df['Text'])\n",
    "    test = tfidf_transformer.transform(test)\n",
    "    \n",
    "    return tfidf, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_params(df):\n",
    "    pipeline, parameters = [dict() for i in range(2)]\n",
    "    #set up cross validation folds\n",
    "    cv = StratifiedShuffleSplit(df.Category, n_iter=5, test_size=.2)\n",
    "\n",
    "    #analysis pipeline with linear svc\n",
    "    pipeline['svc'] = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "        ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "        ('clf', LinearSVC())])\n",
    "\n",
    "    parameters['svc'] = {\n",
    "        'vect__ngram_range': ((1, 1), (1,3), (5, 5)),\n",
    "        'vect__analyzer' : ('char_wb','word'),\n",
    "        'clf__C': (1, 1e3, 1e-3)}\n",
    "\n",
    "    #analysis pipeline with svm with rbf kernel\n",
    "    pipeline['svm'] = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "        ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "        ('clf', SVC())])\n",
    "\n",
    "    parameters['svm'] = {\n",
    "        'vect__ngram_range': ((1, 1), (1,3), (5, 5)),\n",
    "        'vect__analyzer' : ('char_wb','word'),\n",
    "        'clf__C': (1, 1e3, 1e-3),\n",
    "        'clf__gamma': (1, 1e2, 1e-2)}\n",
    "\n",
    "    #analysis pipeline with logistic regression\n",
    "    pipeline['log'] = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "        ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "        ('clf', LogisticRegression(penalty = 'l2', solver = 'lbfgs', multi_class = 'multinomial'))])\n",
    "        \n",
    "    parameters['log'] = {\n",
    "        'vect__ngram_range': ((1, 1), (1,2), (1,3), (5, 5)),\n",
    "        'vect__analyzer' : ('char_wb','word'),\n",
    "        'clf__C': (1, 1e3, 1e-3)}\n",
    "\n",
    "    # analysis pipeline with naive bayes\n",
    "    pipeline['nb'] = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "        ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "        ('clf', MultinomialNB(fit_prior = True))])\n",
    "    parameters['nb'] = {\n",
    "        'vect__ngram_range': ((1, 1), (1,2), (1,3), (5, 5)),\n",
    "        'vect__analyzer' : ('char_wb','word')}\n",
    "\n",
    "    #analysis pipeline with kmeans\n",
    "    pipeline['knn'] = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "        ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "        ('clf', KNeighborsClassifier())])\n",
    "    parameters['knn'] = {\n",
    "        'vect__ngram_range': ((1, 1), (1,2), (1,3), (5, 5)),\n",
    "        'vect__analyzer' : ('char_wb','word'),\n",
    "        'clf__n_neighbors': (8, 9, 10, 15),\n",
    "        'clf__weights': ('uniform', 'distance')}\n",
    "\n",
    "\n",
    "    #fit grid search instance\n",
    "    for m in ['svc','svm', 'log','nb', 'knn']:\n",
    "        scorer = 'accuracy'\n",
    "        gs_clf = GridSearchCV(pipeline[m], parameters[m], verbose = 1, cv = cv, n_jobs = -1, scoring = scorer)\n",
    "\n",
    "        gs_clf = gs_clf.fit(df['TokensNouns'], df.Category)\n",
    "\n",
    "        best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1]) #find best params\n",
    "    \n",
    "        print ('{0}:\\n\\tbest score: {1}\\nparameters: {2}'.format(m, score, best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes in training data, training labels, predicts labels for xtest using three methods.\n",
    "def ensemble_predictor(df, test_df): \n",
    "    mod1 = LinearSVC(C=1) #vec analyzer = word, ngram(1,1)\n",
    "    mod2 = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "    mod3 = LogisticRegression(C=1)#vec analyzer = word, ngram(1,3)\n",
    "    mod4 = KNeighborsClassifier(n_neighbors=15)\n",
    "    mod5 = SVC(C=1, gamma=1)\n",
    "    \n",
    "    ytrain = df['Category']\n",
    "    xtrain1, xtest1 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1,3)))\n",
    "    xtrain2, xtest2 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1,1)))\n",
    "    xtrain3, xtest3 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1,2)))\n",
    "    xtrain4, xtest4 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1, 3)))\n",
    "    #print(df.head())\n",
    "    xtrain5, xtest5 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1, 3)))\n",
    "    \n",
    "    \n",
    "    mod1.fit(xtrain1, ytrain)\n",
    "    mod2.fit(xtrain2, ytrain)\n",
    "    mod3.fit(xtrain3, ytrain)\n",
    "    mod4.fit(xtrain4, ytrain)\n",
    "    mod5.fit(xtrain5, ytrain)\n",
    "    \n",
    "    y1 = mod1.predict(xtest1)\n",
    "    y2 = mod2.predict(xtest2)\n",
    "    y3 = mod3.predict(xtest3)\n",
    "    y4 = mod4.predict(xtest4)\n",
    "    y5 = mod5.predict(xtest5)\n",
    "    \n",
    "    votes = zip(y1, y3, y4, y5)\n",
    "    ypredicted = []\n",
    "    count = 0\n",
    "    for v in votes:\n",
    "        v = list(v)\n",
    "        p = max(set(v), key=v.count)\n",
    "        if len(np.unique(v)) == len(v):\n",
    "            count += 1\n",
    "            p = v[1]\n",
    "        ypredicted.append(p)\n",
    "    return ypredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluate using 10 fold cross-validation, inspect results\n",
    "def cross_validate(targets, df):\n",
    "    \n",
    "    #LinearSVC\n",
    "    mod = LinearSVC(C=1)\n",
    "    cv = StratifiedShuffleSplit(targets, n_iter=10, test_size=.1)\n",
    "    \n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1,3)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "\n",
    "    print('\\nLinear SVC\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    #SVC\n",
    "    mod = SVC(C=1, gamma=1.0)\n",
    "    cv = StratifiedShuffleSplit(targets, n_iter=10, test_size=.1)\n",
    "    \n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='char_wb', ngram_range=(5, 5)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "\n",
    "    print('\\nRBF SVC\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    #naive bayes\n",
    "    mod = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1,2)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nNaive Bayes\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \"\"\"\n",
    "    mod = AdaBoostClassifier()\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1,2)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nAdaBoost\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \"\"\"\n",
    "    mod = LogisticRegression(C=1)\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1,2)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nLogReg\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    mod = KNeighborsClassifier(n_neighbors=15)\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1, 3)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nKNN\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "      \n",
    "    \n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        predictions = ensemble_predictor(train, test)\n",
    "        scores.append(accuracy_score(targets[tt], predictions))\n",
    "    print('\\nEnsemble Predictor\\n\\t mean score: {0}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only do when necessary. Will take 12 hours\n",
    "#find_params(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost\n",
      "\t mean score: 0.6270408163265306\n",
      "\n",
      "LogReg\n",
      "\t mean score: 0.686807580174927\n"
     ]
    }
   ],
   "source": [
    "#train_df.head()\n",
    "cross_validate(train_df['Category'], train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_df = add_features(unknown_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = ensemble_predictor(train_df, unknown_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions).reset_index().head()\n",
    "df.columns = (['ID','Category'])\n",
    "df['ID'] = df['ID']+1\n",
    "df.to_csv('drug_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
